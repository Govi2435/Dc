Linear regression — Error-correction (Delta) rule (online)
import numpy as np

class LinearRegressionDelta:
    """
    Simple linear regression using the error-correction (delta) rule (online).
    Model: y_hat = w.dot(x) + b
    Trained with: w += lr * error * x ; b += lr * error
    """
    def __init__(self, n_features, lr=0.01, seed=None):
        if seed is not None:
            np.random.seed(seed)
        self.w = np.random.randn(n_features) * 0.01
        self.b = 0.0
        self.lr = lr

    def predict(self, X):
        return X.dot(self.w) + self.b

    def partial_fit(self, x, y):
        """Single-sample update (x is 1d array of features)."""
        y_hat = x.dot(self.w) + self.b
        error = y - y_hat
        self.w += self.lr * error * x
        self.b += self.lr * error
        return error

    def fit(self, X, y, epochs=10, shuffle=True, verbose=False):
        n = X.shape[0]
        for epoch in range(epochs):
            idx = np.arange(n)
            if shuffle:
                np.random.shuffle(idx)
            epoch_loss = 0.0
            for i in idx:
                err = self.partial_fit(X[i], y[i])
                epoch_loss += err**2
            epoch_loss /= n
            if verbose:
                print(f"Epoch {epoch+1}/{epochs}  MSE={epoch_loss:.6f}")

# --- Demo with synthetic data ---
def demo_linear():
    np.random.seed(0)
    # generate synthetic linear data: y = 3*x1 - 2*x2 + 5 + noise
    n_samples = 200
    X = np.random.randn(n_samples, 2)
    true_w = np.array([3.0, -2.0])
    true_b = 5.0
    y = X.dot(true_w) + true_b + np.random.randn(n_samples) * 0.5

    # train with delta rule (online)
    model = LinearRegressionDelta(n_features=2, lr=0.01, seed=1)
    model.fit(X, y, epochs=30, verbose=True)

    print("\nLearned parameters (delta rule):")
    print("w =", model.w)
    print("b =", model.b)

    # closed-form solution (normal equation)
    X_aug = np.hstack([X, np.ones((n_samples, 1))])
    # theta = (X^T X)^{-1} X^T y
    theta = np.linalg.pinv(X_aug.T.dot(X_aug)).dot(X_aug.T).dot(y)
    w_closed = theta[:2]
    b_closed = theta[2]
    print("\nClosed-form solution (normal equation):")
    print("w =", w_closed)
    print("b =", b_closed)

    # compare MSE
    y_pred = model.predict(X)
    mse_online = np.mean((y - y_pred)**2)
    y_closed = X.dot(w_closed) + b_closed
    mse_closed = np.mean((y - y_closed)**2)
    print(f"\nMSE (delta rule)  : {mse_online:.6f}")
    print(f"MSE (closed-form) : {mse_closed:.6f}")

if __name__ == "__main__":
    demo_linear()


------------------------------------------------------------------

Logistic regression — Error-correction style update (online)

import numpy as np

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

class LogisticRegressionSGD:
    """
    Logistic regression (binary) trained with online SGD.
    Update rule per sample: w += lr * (y - p) * x ; b += lr * (y - p)
    This is equivalent to gradient descent on log-loss.
    """
    def __init__(self, n_features, lr=0.1, seed=None):
        if seed is not None:
            np.random.seed(seed)
        self.w = np.random.randn(n_features) * 0.01
        self.b = 0.0
        self.lr = lr

    def predict_proba(self, X):
        z = X.dot(self.w) + self.b
        return sigmoid(z)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

    def partial_fit(self, x, y):
        p = sigmoid(x.dot(self.w) + self.b)
        error = y - p  # note: for cross-entropy, gradient is (p - y), so we step with (y - p)
        self.w += self.lr * error * x
        self.b += self.lr * error
        # return negative log-likelihood (loss) for monitoring
        eps = 1e-12
        loss = - (y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))
        return loss

    def fit(self, X, y, epochs=10, shuffle=True, verbose=False):
        n = X.shape[0]
        for epoch in range(epochs):
            idx = np.arange(n)
            if shuffle:
                np.random.shuffle(idx)
            epoch_loss = 0.0
            for i in idx:
                l = self.partial_fit(X[i], y[i])
                epoch_loss += l
            epoch_loss /= n
            if verbose:
                preds = self.predict(X)
                acc = np.mean(preds == y)
                print(f"Epoch {epoch+1}/{epochs}  AvgLoss={epoch_loss:.4f}  Acc={acc:.4f}")

# --- Demo with synthetic binary classification data ---
def demo_logistic():
    np.random.seed(42)
    n = 400
    # create two gaussian clusters separable by a linear boundary
    X_class0 = np.random.randn(n//2, 2) + np.array([-2.0, 0.0])
    X_class1 = np.random.randn(n//2, 2) + np.array([2.0, 0.0])
    X = np.vstack([X_class0, X_class1])
    y = np.hstack([np.zeros(n//2), np.ones(n//2)])

    # train logistic regression with SGD (error-correction style)
    model = LogisticRegressionSGD(n_features=2, lr=0.1, seed=0)
    model.fit(X, y, epochs=40, verbose=True)

    # evaluate
    probs = model.predict_proba(X)
    preds = model.predict(X)
    acc = np.mean(preds == y)
    print("\nFinal accuracy:", acc)
    print("Weights:", model.w, "Bias:", model.b)

    # quick demonstration of probabilities for first 10 samples
    print("\nFirst 10 samples: true -> pred_prob -> pred_label")
    for i in range(10):
        print(f"{int(y[i])} -> {probs[i]:.3f} -> {preds[i]}")

if __name__ == "__main__":
    demo_logistic()
