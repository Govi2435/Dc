"""
gridworld_td.py

Introduction to GridWorld and Temporal-Difference (TD(0)) learning.

This script includes:
 - A simple GridWorld environment (deterministic moves with walls/terminal states)
 - TD(0) policy evaluation for a given (possibly random) policy
 - SARSA (on-policy TD control) to learn an action-value function and a policy

Reference / uploaded lab sheet: /mnt/data/MLDL experiment list.pdf
"""

import numpy as np
import random
import matplotlib.pyplot as plt

# -----------------------
# GridWorld environment
# -----------------------
class GridWorld:
    """
    Simple GridWorld:
      - grid: shape (H, W)
      - start: starting cell (r,c)
      - terminals: dict {(r,c): reward}
      - obstacles: set of cells that are blocked (can't enter)
      - actions: 0=up,1=right,2=down,3=left
      - transition: deterministic (if move hits wall/obstacle, stay in place)
    """
    ACTIONS = [( -1,  0),  # up
               (  0,  1),  # right
               (  1,  0),  # down
               (  0, -1)]  # left

    ACTION_NAMES = ['U', 'R', 'D', 'L']

    def __init__(self, height, width, start=(0,0), terminals=None, obstacles=None, default_reward=-0.04):
        self.H = height
        self.W = width
        self.start = start
        self.default_reward = default_reward
        self.terminals = dict(terminals) if terminals else {}  # {(r,c): reward}
        self.obstacles = set(obstacles) if obstacles else set()
        self.reset()

    def in_bounds(self, r, c):
        return 0 <= r < self.H and 0 <= c < self.W

    def is_terminal(self, s):
        return s in self.terminals

    def step(self, state, action):
        """Take action (0..3) from state (r,c), return next_state, reward, done"""
        if self.is_terminal(state):
            return state, 0.0, True

        dr, dc = GridWorld.ACTIONS[action]
        nr, nc = state[0] + dr, state[1] + dc
        # check obstacles / bounds
        if (not self.in_bounds(nr, nc)) or ((nr, nc) in self.obstacles):
            nr, nc = state  # stay in place
        next_state = (nr, nc)
        reward = self.terminals.get(next_state, self.default_reward)
        done = self.is_terminal(next_state)
        return next_state, reward, done

    def reset(self):
        self.state = self.start
        return self.state

    def all_states(self):
        """List all non-obstacle grid coordinates"""
        return [(r, c) for r in range(self.H) for c in range(self.W) if (r, c) not in self.obstacles]

    def state_to_idx(self, s):
        return s[0] * self.W + s[1]

    def idx_to_state(self, idx):
        return (idx // self.W, idx % self.W)

    def render_values(self, V, title="Value function"):
        """Plot V as grid heatmap (V is dict (r,c)->value)"""
        grid = np.full((self.H, self.W), np.nan)
        for r in range(self.H):
            for c in range(self.W):
                if (r,c) in self.obstacles:
                    grid[r,c] = np.nan
                else:
                    grid[r,c] = V.get((r,c), 0.0)
        plt.figure(figsize=(self.W, self.H))
        plt.imshow(grid, cmap='coolwarm', interpolation='nearest')
        plt.colorbar()
        for r in range(self.H):
            for c in range(self.W):
                if (r,c) in self.obstacles:
                    plt.text(c, r, "X", ha='center', va='center', color='k', fontsize=16)
                elif (r,c) in self.terminals:
                    plt.text(c, r, f"T\n{self.terminals[(r,c)]:.2f}", ha='center', va='center', color='k', fontsize=10)
                else:
                    plt.text(c, r, f"{V.get((r,c),0.0):.2f}", ha='center', va='center', color='k', fontsize=9)
        plt.title(title)
        plt.gca().invert_yaxis()
        plt.show()

    def render_policy(self, policy, title="Policy"):
        """policy: dict (r,c) -> action index (or list of action probabilities)"""
        arrow_map = {0: '↑', 1: '→', 2: '↓', 3: '←'}
        plt.figure(figsize=(self.W, self.H))
        plt.imshow(np.zeros((self.H, self.W)), cmap='gray', interpolation='nearest')
        for r in range(self.H):
            for c in range(self.W):
                if (r,c) in self.obstacles:
                    txt = "X"
                elif (r,c) in self.terminals:
                    txt = "T"
                else:
                    a = policy.get((r,c))
                    if a is None:
                        txt = '?'
                    elif isinstance(a, (list, tuple, np.ndarray)):
                        # show greedy action
                        ga = int(np.argmax(a))
                        txt = arrow_map.get(ga, '?')
                    else:
                        txt = arrow_map.get(a, '?')
                plt.text(c, r, txt, ha='center', va='center', color='red', fontsize=16)
        plt.title(title)
        plt.gca().invert_yaxis()
        plt.axis('off')
        plt.show()


# -----------------------
# TD(0) Policy Evaluation
# -----------------------
def td0_policy_evaluation(env, policy, gamma=0.99, alpha=0.1, episodes=500):
    """
    Evaluate a given (possibly stochastic) policy using TD(0) (online).
    - env: GridWorld
    - policy: function(state)->action or dict mapping to action probabilities
      If policy returns an action index (int) it's deterministic.
      If policy[state] is a list/array of action probabilities, sample accordingly.
    """
    # initialize V(s) arbitrarily (0)
    V = {s: 0.0 for s in env.all_states()}
    for ep in range(episodes):
        state = env.reset()
        done = False
        while not done:
            # choose action
            if callable(policy):
                action = policy(state)
            else:
                p = policy.get(state)
                if p is None:
                    action = np.random.choice(len(GridWorld.ACTIONS))
                elif isinstance(p, (list, tuple, np.ndarray)):
                    action = np.random.choice(len(GridWorld.ACTIONS), p=p)
                else:
                    action = p
            next_state, reward, done = env.step(state, action)
            # TD(0) update: V(s) <- V(s) + alpha * (r + gamma V(s') - V(s))
            V[state] = V[state] + alpha * (reward + gamma * V.get(next_state, 0.0) - V[state])
            state = next_state
    return V

# -----------------------
# SARSA (on-policy TD control)
# -----------------------
def epsilon_greedy_action(Q, state, epsilon=0.1, n_actions=4):
    if random.random() < epsilon:
        return random.randrange(n_actions)
    else:
        q = [Q.get((state,a), 0.0) for a in range(n_actions)]
        return int(np.argmax(q))

def sarsa_control(env, num_episodes=2000, alpha=0.1, gamma=0.99, epsilon=0.1):
    """
    SARSA algorithm to learn action-value function Q(s,a) and policy.
    Returns Q and derived greedy policy.
    """
    Q = {}  # mapping (state, action) -> value
    # initialize Q to zero for all state-action pairs
    for s in env.all_states():
        for a in range(len(GridWorld.ACTIONS)):
            Q[(s,a)] = 0.0

    for ep in range(num_episodes):
        state = env.reset()
        action = epsilon_greedy_action(Q, state, epsilon=epsilon)
        done = False
        while not done:
            next_state, reward, done = env.step(state, action)
            next_action = epsilon_greedy_action(Q, next_state, epsilon=epsilon)
            # SARSA update
            Q[(state, action)] += alpha * (reward + gamma * Q.get((next_state, next_action), 0.0) - Q[(state, action)])
            state, action = next_state, next_action
    # derive greedy policy
    policy = {}
    for s in env.all_states():
        if env.is_terminal(s):
            continue
        q_vals = np.array([Q[(s,a)] for a in range(len(GridWorld.ACTIONS))])
        policy[s] = int(np.argmax(q_vals))
    return Q, policy

# -----------------------
# Small demo & usage
# -----------------------
def demo_td_and_sarsa():
    # Build a simple 4x4 GridWorld as in classic examples
    # Terminal states at (0,3)=+1 and (1,3)=-1
    H, W = 4, 4
    terminals = { (0,3): 1.0, (1,3): -1.0 }
    obstacles = None
    gw = GridWorld(H, W, start=(3,0), terminals=terminals, obstacles=obstacles, default_reward=-0.04)

    print("GridWorld created: size", H, "x", W)
    print("Terminals:", terminals)

    # Define a random policy (uniform random)
    uniform_policy = {s: np.ones(len(GridWorld.ACTIONS)) / len(GridWorld.ACTIONS) for s in gw.all_states() if not gw.is_terminal(s)}

    # 1) TD(0) policy evaluation for random policy
    print("\nRunning TD(0) policy evaluation (random policy)...")
    V = td0_policy_evaluation(gw, uniform_policy, gamma=0.9, alpha=0.1, episodes=1000)
    gw.render_values(V, title="TD(0) - Value function (random policy)")

    # Show policy (random -> arrows will be random; we display '?'/random)
    gw.render_policy(uniform_policy, title="Random policy (displayed as greedy of probabilities)")

    # 2) SARSA control to learn a policy
    print("\nRunning SARSA (on-policy TD control)...")
    Q, policy = sarsa_control(gw, num_episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1)
    # Value from Q for visualization (V(s) = max_a Q(s,a) or expected under policy)
    V_from_Q = { s: max([Q[(s,a)] for a in range(len(GridWorld.ACTIONS))]) for s in gw.all_states() }
    gw.render_values(V_from_Q, title="Value (from learnt Q) after SARSA")
    gw.render_policy(policy, title="Greedy policy derived from SARSA")

    # Print Q for start state
    start = gw.start
    print("\nQ-values for start state", start)
    for a in range(len(GridWorld.ACTIONS)):
        print(f"  action {GridWorld.ACTION_NAMES[a]} : {Q[(start,a)]:.3f}")

if __name__ == "__main__":
    demo_td_and_sarsa()
